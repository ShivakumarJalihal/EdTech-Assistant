A Large Language Model (LLM) is a type of artificial intelligence model designed to understand, generate, and process human language. These models are 'large' because they are trained on vast amounts of text data and have billions of parameters. The core architecture of most modern LLMs is the Transformer, which uses a mechanism called 'attention' to weigh the importance of different words in a sequence. This allows them to handle long-range dependencies in text effectively. LLMs are pre-trained on a general corpus of text and can then be fine-tuned for specific tasks like translation, summarization, or question answering.